{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VulXquoOxhD"
   },
   "source": [
    "# IMPORT LIBRARIES AND DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16032,
     "status": "ok",
     "timestamp": 1594741681161,
     "user": {
      "displayName": "Kukeshajanth Kodeswaran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_hF0QlKjkAZvh3_nIU1Fj3LuIyLifAN3KKIdI7A=s64",
      "userId": "01021579274124875186"
     },
     "user_tz": 240
    },
    "id": "bpiddPjsl_4Q",
    "outputId": "cf7c418c-99f9-4de2-d833-539df0bba063"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, RepeatVector, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n",
    "# setting the style of the notebook to be monokai theme  \n",
    "# this line of code is important to ensure that we are able to see the x and y axes clearly\n",
    "# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QiTczEunJNx"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_english = pd.read_csv('small_vocab_en.csv', sep = '/t', names = ['english'])\n",
    "df_french = pd.read_csv('small_vocab_fr.csv', sep = '/t', names = ['french'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1594744193257,
     "user": {
      "displayName": "Kukeshajanth Kodeswaran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_hF0QlKjkAZvh3_nIU1Fj3LuIyLifAN3KKIdI7A=s64",
      "userId": "01021579274124875186"
     },
     "user_tz": 240
    },
    "id": "PgTEJ5aGpHfU",
    "outputId": "61760e9a-dca0-481f-d4df-e2c297140154"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_english, df_french], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgkZHO3CPnsp"
   },
   "source": [
    "# PERFORM DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk packages\n",
    "nltk.download('punkt')\n",
    "\n",
    "# download stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1594744220721,
     "user": {
      "displayName": "Kukeshajanth Kodeswaran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_hF0QlKjkAZvh3_nIU1Fj3LuIyLifAN3KKIdI7A=s64",
      "userId": "01021579274124875186"
     },
     "user_tz": 240
    },
    "id": "YDtZAF0l8qx4",
    "outputId": "812270ff-2ec0-4711-d2fe-6b083c2cf372"
   },
   "outputs": [],
   "source": [
    "# function to remove punctuations\n",
    "def remove_punc(x):\n",
    "    return re.sub('[!#?,.:\";]', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['french'] = df['french'].apply(remove_punc)\n",
    "df['english'] = df['english'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = []\n",
    "french_words  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(x, word_list):\n",
    "    for word in x.split():\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "\n",
    "df[\"english\"].apply(lambda x: get_unique_words(x, english_words));\n",
    "df[\"french\"].apply(lambda x: get_unique_words(x, french_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words in french\n",
    "total_english_words = len(english_words)\n",
    "total_french_words = len(french_words)\n",
    "print(total_english_words)\n",
    "print(total_french_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf4bOLzfPc78"
   },
   "source": [
    "# VISUALIZE CLEANED UP DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of all words in the dataset\n",
    "words = []\n",
    "for i in df['english']:\n",
    "    for word in i.split():\n",
    "        words.append(word)\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total count of words\n",
    "english_words_counts = Counter(words)\n",
    "english_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary by values\n",
    "english_words_counts = sorted(english_words_counts.items(), key = operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words, english_counts = zip(*english_words_counts)\n",
    "english_words = list(english_words)\n",
    "english_counts = list(english_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # append the values to a list for visualization purposes\n",
    "# english_words = []\n",
    "# english_counts = []\n",
    "# for i in range(len(english_words_counts)):\n",
    "#     english_words.append(english_words_counts[i][0])\n",
    "#     english_counts.append(english_words_counts[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barplot using plotly \n",
    "fig = px.bar(x = english_words, y = english_counts)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is Real\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000, width = 1600, height = 800 ).generate(\" \".join(df.english))\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.english[0]\n",
    "nltk.word_tokenize(df.english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length (number of words) per document. We will need it later for embeddings\n",
    "maxlen_english = -1\n",
    "for doc in df.english:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen_english < len(tokens)):\n",
    "        maxlen_english = len(tokens)\n",
    "print(\"The maximum number of words in any document = \", maxlen_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of all words in the dataset\n",
    "words = []\n",
    "for i in df['french']:\n",
    "    for word in i.split():\n",
    "        words.append(word)\n",
    "    \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the total count of words\n",
    "french_words_counts = Counter(words)\n",
    "french_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary by values\n",
    "french_words_counts = sorted(french_words_counts.items(), key = operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words, french_counts = zip(*french_words_counts)\n",
    "french_words = list(french_words)\n",
    "french_counts = list(french_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barplot using plotly \n",
    "fig = px.bar(x = french_words, y = french_counts)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the word cloud for text that is Real\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000, width = 1600, height = 800 ).generate(\" \".join(df.french))\n",
    "plt.imshow(wc, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.french[0]\n",
    "nltk.word_tokenize(df.french[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length (number of words) per document. We will need it later for embeddings\n",
    "maxlen_french = -1\n",
    "for doc in df.french:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen_french < len(tokens)):\n",
    "        maxlen_french = len(tokens)\n",
    "print(\"The maximum number of words in any document = \", maxlen_french)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi3neznbP_QT"
   },
   "source": [
    "# PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad(x, maxlen):\n",
    "    #  a tokenier to tokenize the words and create sequences of tokenized words\n",
    "    tokenizer = Tokenizer(char_level = False)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    padded = pad_sequences(sequences, maxlen = maxlen, padding = 'post')\n",
    "    \n",
    "    return tokenizer, sequences, padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and padding to the data \n",
    "maxlen = max(maxlen_english, maxlen_french)\n",
    "x_tokenizer, x_sequences, x_padded = tokenize_and_pad(df.english, maxlen)\n",
    "y_tokenizer, y_sequences, y_padded = tokenize_and_pad(df.french,  maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total vocab size, since we added padding we add 1 to the total word count\n",
    "english_vocab_size = total_english_words + 1\n",
    "print(\"Complete English Vocab Size:\", english_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total vocab size, since we added padding we add 1 to the total word count\n",
    "french_vocab_size = total_french_words + 1\n",
    "print(\"Complete French Vocab Size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The tokenized version for document\\n\", df.english[-1:].item(),\"\\n is : \", x_padded[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The tokenized version for document\\n\", df.french[-1:].item(),\"\\n is : \", y_padded[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain the text from padded variables\n",
    "def pad_to_text(padded, tokenizer):\n",
    "\n",
    "    id_to_word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "\n",
    "    return ' '.join([id_to_word[j] for j in padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_to_text(y_padded[0], y_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHckYpNqQaMf"
   },
   "source": [
    "# BUILD AND TRAIN THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259747,
     "status": "ok",
     "timestamp": 1594742074176,
     "user": {
      "displayName": "Kukeshajanth Kodeswaran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_hF0QlKjkAZvh3_nIU1Fj3LuIyLifAN3KKIdI7A=s64",
      "userId": "01021579274124875186"
     },
     "user_tz": 240
    },
    "id": "Z0d7qNMLUCc3",
    "outputId": "73763673-5405-4eea-b841-cb15edfafd24"
   },
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "model = Sequential()\n",
    "# embedding layer\n",
    "model.add(Embedding(english_vocab_size, 256, input_length = maxlen, mask_zero = True))\n",
    "# encoder\n",
    "model.add(LSTM(256))\n",
    "# decoder\n",
    "# repeatvector repeats the input for the desired number of times to change\n",
    "# 2D-array to 3D array. For example: (1,256) to (1,23,256)\n",
    "model.add(RepeatVector(maxlen))\n",
    "model.add(LSTM(256, return_sequences= True ))\n",
    "model.add(TimeDistributed(Dense(french_vocab_size, activation ='softmax')))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 756753,
     "status": "ok",
     "timestamp": 1594742572420,
     "user": {
      "displayName": "Kukeshajanth Kodeswaran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_hF0QlKjkAZvh3_nIU1Fj3LuIyLifAN3KKIdI7A=s64",
      "userId": "01021579274124875186"
     },
     "user_tz": 240
    },
    "id": "SEdds6i0-ohn",
    "outputId": "a28adf0d-496c-43ff-c86e-19e059c6ddf3"
   },
   "outputs": [],
   "source": [
    "# change the shape of target from 2D to 3D\n",
    "y_train = np.expand_dims(y_train, axis = 2)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(x_train, y_train, batch_size=1024, validation_split= 0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from plot_model import plot_model\n",
    "plot_model(\n",
    "    model, \n",
    "    to_file='model.png', \n",
    "    show_shapes=True, \n",
    "    show_layer_names=False, \n",
    "    rankdir='TB', \n",
    "    expand_nested=False, \n",
    "    style=0, \n",
    "    color=True, \n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXa2MqEQQ2ww"
   },
   "source": [
    "# ASSESS TRAINED MODEL PERFORMANCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make prediction\n",
    "def prediction(x, x_tokenizer = x_tokenizer, y_tokenizer = y_tokenizer):\n",
    "    predictions = model.predict(x)[0]\n",
    "    id_to_word = {id: word for word, id in y_tokenizer.word_index.items()}\n",
    "    id_to_word[0] = ''\n",
    "    return ' '.join([id_to_word[j] for j in np.argmax(predictions,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Original English word - {}\\n'.format(pad_to_text(x_test[i], x_tokenizer)))\n",
    "    print('Original French word - {}\\n'.format(pad_to_text(y_test[i], y_tokenizer)))\n",
    "    print('Predicted French word - {}\\n\\n\\n\\n'.format(prediction(x_test[i:i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMaP+zcpR2ZiRWod+z33Yf5",
   "collapsed_sections": [],
   "mount_file_id": "1H7gdt6J40zOB2rJugzLowy5j5DjIAg6z",
   "name": "FakeNewsClassiication.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
