{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective : To apply Sequential model to machine translations using IITB Hindi English Corpus\n",
    "####  1. In this notebook we will create a simple Encoder using 2 Layers Lstm with dropouts and input the hidden state and cell state time stamp wise to decoder\n",
    "####  2. Our decoder architecture will take the hidden and cell state from encoder and sentence as a input to do the predictions\n",
    "####  3. In the Seq2 Seq architecture we will use combination of teacher forcing and decoder output for training at each step. Teacher forcing speeds up the convergence of the algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "\n",
    "The IIT Bombay English-Hindi corpus contains parallel corpus for English-Hindi as well as monolingual Hindi corpus collected from a variety of existing sources and corpora developed at the Center for Indian Language Technology, IIT Bombay over the years. This page describes the corpus. This corpus has been used at the Workshop on Asian Language Translation Shared Task in 2016 and 2017 for the Hindi-to-English and English-to-Hindi languages pairs and as a pivot language pair for the Hindi-to-Japanese and Japanese-to-Hindi language pairs.\n",
    "\n",
    "\n",
    "Please refer to the : http://www.cfilt.iitb.ac.in/iitb_parallel/ for more details on the datasource and downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #### for any manipulations on the dataframe\n",
    "import os #### For doing os operations like change directory and all\n",
    "\n",
    "import spacy ### We will use spacy tokeniser for cleaning our data\n",
    "import numpy as np ### For any numeric  operations of matrices\n",
    "\n",
    "\n",
    "#### We will import the torch and will start working on the project\n",
    "import torch\n",
    "from torchtext import data ### This provides the pipeline for processing our data\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True ### This will make our experiments reproducible\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim ### We will call our optimiser function like adam, sgd etcc....\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import pyprind\n",
    "%matplotlib inline  \n",
    "import time\n",
    "import re ### This will help us in writing regex for cleaning our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\ashwinku\\\\Desktop\\\\Pytorch\\\\Neural_Machine_Translation\\\\Data\\\\parallel\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function which takes english file hindi file and return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(englishfile,hindifile):\n",
    "    ''' Takes 2 inputs english file and hindi file data returns the dataframe name'''\n",
    "    filename = open(englishfile,encoding='utf-8') ### Store the file inot i/0 iterator\n",
    "    english_list = list(filename) ## convert the english file to list of documents\n",
    "    filename1 = open(hindifile,encoding = 'utf-8') ### Store the file as i/o iterator\n",
    "    hindi_list = list(filename1) ## convert the hindi file to list of documents\n",
    "    print (\"Length of hindi list is :\",len(hindi_list))\n",
    "    print (\"Length of english list is :\",len(english_list))\n",
    "    # dictionary of lists  \n",
    "    cols_dict = {'hindi_text': hindi_list, 'english_text':english_list}\n",
    "    ## Craete the dataframe from cols dict\n",
    "    df_name = pd.DataFrame(cols_dict)\n",
    "    print (\"Shape of data is :\",df_name.shape)\n",
    "    print (\"Columns of data is :\",df_name.columns)\n",
    "    return (df_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the functions on Training dataset and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of hindi list is : 1561840\n",
      "Length of english list is : 1561840\n",
      "Shape of data is : (1561840, 2)\n",
      "Columns of data is : Index(['hindi_text', 'english_text'], dtype='object')\n",
      "Length of hindi list is : 520\n",
      "Length of english list is : 520\n",
      "Shape of data is : (520, 2)\n",
      "Columns of data is : Index(['hindi_text', 'english_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hin_en_df = readfile('IITB.en-hi.en', 'IITB.en-hi.hi')\n",
    "dev_en_df = readfile('dev.en','dev.hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the dataframe and check some random values to make sure, data is aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi_text       बनाएँ\\n\n",
      "english_text    Create\\n\n",
      "Name: 10000, dtype: object\n",
      "hindi_text         पुनरावर्ती आमवात आक्रमण प्राय एक से तीन संधियों को सम्मिलित करता है\\n\n",
      "english_text    Attacks of palindromic rheumatism usually involve one to three joints.\\n\n",
      "Name: 987654, dtype: object\n",
      "hindi_text        हमें पाठ्यक्रम में विषय के रूप में शामिल करके मानव अधिकारों के प्रति जागरूकता बढ़ानी चाहिए।\\n\n",
      "english_text    We must increase awareness for human rights by including it as a subject in school curricula.\\n\n",
      "Name: 1560000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "print (hin_en_df.iloc[10000,[0,1]])\n",
    "print (hin_en_df.iloc[987654,[0,1]])\n",
    "print (hin_en_df.iloc[1560000,[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets  create a pipeline using 10% data only for now\n",
    "\n",
    "dev_en_df.to_csv(\"dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As we have only 8GB GPU, we were getting memory error on more than 20% data\n",
    "    1. We are using random sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the 20% sample and store it as csv for preocssing in model\n",
    "hin_en_df.sample(frac =0.20,random_state=1234).to_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312368, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"sample.csv\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will define the data cleaning pipeline for hindi as well as english\n",
    "    1. Maybe for hindi we have want to use hindi tokeniser from indic-nlp library but initially lets use nlp as it is tokenising correctly\n",
    "    2. Do a little bit of cleaning on english and hindi text seprately\n",
    "    3. We will use this tokenisation as function for processing data in torch text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will use NLP tokeniser and disable parser and other functionalties for speed\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "###  Define the tokenizer function to be used later on\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(corp_clean(s))]\n",
    "\n",
    "### Every token should be cleanedbefore going through the process\n",
    "def corp_clean(text):\n",
    "#     text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    text = text.replace(\"\\\\\",\" \")\n",
    "    text = re.sub(r'/n',' ',text)\n",
    "#     print (\"English text is \",text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def tokenizer_hindi(j):\n",
    "    return [w.lower().strip() for w in hindi_clean(j).split(\" \") if w != '']\n",
    "\n",
    "def hindi_clean(text1):\n",
    "    text1 = re.sub(r'https?:/\\/\\S+', ' ', text1)\n",
    "    text1 = text1.replace(\"\\\\\",\" \")\n",
    "    text1 = re.sub(r'\\n',' ',text1)\n",
    "    text1 = re.sub('।',' । ',text1)\n",
    "#     print (\"hindi token is :\",text1)\n",
    "    return text1.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data cleaning pipeline using torchtext Data nd tabular dataets\n",
    "    1. Add <sos> and <eos> tokens to english as well as hindi transaltions\n",
    "    2. Use different tokeniser for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using Data field function from torchtext we candefine how ew ant to process out data\n",
    "\n",
    "### Definition for processing text field\n",
    "eng_field = data.Field(sequential=True, init_token = '<sos>',eos_token = '<eos>',\n",
    "                       tokenize=tokenizer,  use_vocab=True )\n",
    "\n",
    "### Definition for processing label field\n",
    "hindi_field = data.Field(sequential=True,init_token = '<sos>',eos_token = '<eos>', ##Whether the datatype represents sequential data\n",
    "                       tokenize=tokenizer_hindi,  use_vocab=True)\n",
    "\n",
    "\n",
    "### Define which field in csv is label field and which one is text field\n",
    "train_val_fields = [('unnamed', None), # we dont need this, so no processing\n",
    "    ('hindi_text', hindi_field), # process it as label\n",
    "    ('eng_text', eng_field), # we dont need this, so no processing\n",
    "                   ]\n",
    "\n",
    "### We will read the tabular data and craete split from it\n",
    "train, test = data.TabularDataset.splits(path='C:\\\\Users\\\\ashwinku\\\\Desktop\\\\Pytorch\\\\Neural_Machine_Translation\\\\Data\\\\parallel\\\\', \n",
    "                                            format='csv', \n",
    "                                            train='sample.csv', \n",
    "                                            validation='dev.csv', \n",
    "                                            fields=train_val_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create the vocab for both hindi and english. In hindi we have taken threshold to be high to reduce the vocab size and subsequently th size of the model input and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The number of distinct vocab is : 99971\n",
      " The number of distinct vocab is : 209135\n",
      "Indice of word the is : 2\n"
     ]
    }
   ],
   "source": [
    "### Build the vocabulary using the embeddings\n",
    "eng_field.build_vocab(train, min_freq = 2)\n",
    "# build vocab for labels\n",
    "hindi_field.build_vocab(train,min_freq = 30)\n",
    "\n",
    "### Store the pretrained embedding as model embedding weigh data and make it untrainable\n",
    "#### Print look at the frequency of dat\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "#### print the length of text field vocab\n",
    "print (\" The number of distinct vocab is :\",len(eng_field.vocab.freqs))\n",
    "#### print the length of text field vocab\n",
    "print (\" The number of distinct vocab is :\",len(hindi_field.vocab.freqs))\n",
    "\n",
    "#### Checking the indices fo varius words\n",
    "print (\"Indice of word the is :\",hindi_field.vocab.stoi['<sos>'])\n",
    "\n",
    "\n",
    "\n",
    "### Craete the batch iterator\n",
    "### Create an iterator over Batch of data\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(datasets=(train, test), # specify train and validation Tabulardataset\n",
    "                                            batch_size = 32,  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.eng_text), # on what attribute the text should be sorted\n",
    "                                            device=device, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets start defining the model\n",
    "1. An endcoder with LSTM layers, whose hidden and cell state are used in the decoder.\n",
    "2. Cell state and hidden state as the initial state of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence 2 sequece Models\n",
    "1. receiving the input/source sentence\n",
    "2. using the encoder to produce the context vectors\n",
    "3. using the decoder to produce the predicted output/target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model parameters and load the model to cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(eng_field.vocab)\n",
    "OUTPUT_DIM = len(hindi_field.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the paarmeters between -0.08 and 0.08 this is based on seq2seq model paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(53098, 128)\n",
       "    (rnn): LSTM(128, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9711, 128)\n",
       "    (rnn): LSTM(128, 256, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=9711, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,378,479 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_pad_idx = hindi_field.vocab.stoi[hindi_field.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = hindi_pad_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.eng_text\n",
    "        trg = batch.hindi_text\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 25m 46s\n",
      "\tTrain Loss: 4.516 | Train PPL:  91.445\n",
      "Epoch: 02 | Time: 25m 39s\n",
      "\tTrain Loss: 3.948 | Train PPL:  51.854\n",
      "Epoch: 03 | Time: 25m 39s\n",
      "\tTrain Loss: 3.685 | Train PPL:  39.835\n",
      "Epoch: 04 | Time: 25m 40s\n",
      "\tTrain Loss: 3.519 | Train PPL:  33.762\n",
      "Epoch: 05 | Time: 25m 57s\n",
      "\tTrain Loss: 3.403 | Train PPL:  30.042\n",
      "Epoch: 06 | Time: 25m 59s\n",
      "\tTrain Loss: 3.319 | Train PPL:  27.626\n",
      "Epoch: 07 | Time: 26m 0s\n",
      "\tTrain Loss: 3.250 | Train PPL:  25.795\n",
      "Epoch: 08 | Time: 26m 2s\n",
      "\tTrain Loss: 3.196 | Train PPL:  24.435\n",
      "Epoch: 09 | Time: 26m 5s\n",
      "\tTrain Loss: 3.149 | Train PPL:  23.318\n",
      "Epoch: 10 | Time: 26m 2s\n",
      "\tTrain Loss: 3.109 | Train PPL:  22.408\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "import math\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "#     valid_loss = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'hi_en_model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a basic model we can improve following in further excercise\n",
    "1. Using only one context vector i.e. hidden layer and reduce the complexity of model\n",
    "2. Apply attention to improve the model performance\n",
    "3. Try some pretarined Neural Machine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
